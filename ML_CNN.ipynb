{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af3d0faf-5902-4511-870e-bb66b450e8bc",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97d28599-a11e-490c-91ae-13c082e52aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers ## For adding layers\n",
    "from tensorflow.keras.datasets import imdb ## IMDB review data\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences ## For padding\n",
    "import numpy as np ## For shape check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8039e650-2d97-48df-a380-3fdf5c577486",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9ad94e5-073f-4f92-828f-d934ec1ad920",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000  # Consider the top 10,000 most frequent words\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocab_size)  ## Divide into train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5cd93a-591f-4894-96e8-cdb000e10d5e",
   "metadata": {},
   "source": [
    "* At this stage, the train_data variable contains movie reviews, each review words mapped to some indices.\n",
    "* For example, if the review is 'This is a great movie', then the train_data can have values like [44,31,21,18,11], 1 numerical value for each word.\n",
    "* Say for example if the word 'movie' is not in first 10000 (vocab_size) frequent words, then it is not taken into account and the train data value might be [44,31,21,18].\n",
    "* Also, there can be some outliner, for example one review value might be [100011,11,21,35], here 100011 is greater than vocab_size, so it should be eliminated and for that we will use the following function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7444edde-fb63-4de1-bfb8-da35639b0210",
   "metadata": {},
   "source": [
    "# Function to remove outliners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f74a4e58-3a62-4cde-b281-545d852cf37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(data):\n",
    "    ##If word_id, which is the indice value of word, is less than vocab_size, then only it is included in train data, otherwise skipped.\n",
    "    return [[word_id for word_id in review if word_id < vocab_size] for review in data]  \n",
    "\n",
    "train_data = filter_data(train_data) ## Apply the function to train data\n",
    "test_data = filter_data(test_data)   ## Apply the function to test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb7b69a-a671-4cc9-8885-b431ad73d40f",
   "metadata": {},
   "source": [
    "* We can verify if the function works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04d20d24-fa51-435a-91ff-986a5e502350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max index in filtered train_data: 9999\n",
      "Min index in filtered train_data: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Max index in filtered train_data: {max(max(review) for review in train_data)}\")\n",
    "print(f\"Min index in filtered train_data: {min(min(review) for review in train_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cd54ac-69a5-42d1-a62a-f4af74b69664",
   "metadata": {},
   "source": [
    "* Here the max index is less than vocab_size, so the function works in removing the outliners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4d6594-ddc1-4a17-b6ee-511098bd8e53",
   "metadata": {},
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ffdc32-abc0-46f8-9070-c9950cba4140",
   "metadata": {},
   "source": [
    "* Now, if we need to make each review of the same word limit.\n",
    "* So we will call the pad_sequence function and provide input as train_data and put maximum length of the review as needed (max_length).\n",
    "* What it does is, it will add 0 if the review is less than 250 words.\n",
    "* In our example of 'This is a great movie', after padding, the vector would look like [44,31,21,18,0,0,0........] (remember that 'movie' is not in frequent words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "591cc89d-60ca-4aae-b6a2-768a1daa2c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 250  ## Set maximum length for padding\n",
    "train_data = pad_sequences(train_data, maxlen=max_length, padding='post')  ## Apply the padding function for train data\n",
    "test_data = pad_sequences(test_data, maxlen=max_length, padding='post')   ## Apply the padding function for test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1298e4de-ef31-4445-81eb-5b1291ee6753",
   "metadata": {},
   "source": [
    "* Lets check the shape of the padded train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fb63914-a87b-416b-81c6-e1c42c9e01be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 250)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98536b11-b09a-4eac-9291-3a501aead6ba",
   "metadata": {},
   "source": [
    "* This implies that there are 25000 reviews in padded train data and each review contains 250 words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117cb2fc-8dde-464d-88bd-9489606f4cb6",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0848b0f2-0136-4d9f-9a13-f50315fdf578",
   "metadata": {},
   "source": [
    "* Lets create the model and we will use the Sequential class. In TensorFlow, tf.keras.Sequential is a class that represents a linear stack of layers. In other words, it defines a neural network model as a sequence of layers where each layer has exactly one input tensor and one output tensor. The data flows sequentially through these layers from input to output.\n",
    "* Here, we will be using 5 layers and details of each layers are given below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afc47b6-51fa-4a35-b4b3-c46ecba8763e",
   "metadata": {},
   "source": [
    "## Layer 1 - Embedding:\n",
    "An embedding is a mapping from discrete objects, such as words or identifiers, to vectors of real numbers. In NLP, word embeddings are vector representations of words in a continuous vector space, capturing semantic relationships between words.\n",
    "\n",
    "### Usage in TensorFlow:\n",
    "In TensorFlow, the `tf.keras.layers.Embedding` layer creates word embeddings. It takes integer indices representing words as input and outputs dense vectors (embeddings) for each word.\n",
    "\n",
    "### Parameters:\n",
    "- `input_dim`: Size of the vocabulary, i.e., total number of unique words.\n",
    "- `output_dim`: Dimensionality of the dense embedding vectors.\n",
    "- `input_length`: Length of input sequences.\n",
    "\n",
    "### How it Works:\n",
    "1. **Internal Lookup Table Creation:** The layer initializes an internal lookup table with shape `(vocab_size, output_dim)` mapping word indices to embedding vectors.\n",
    "2. **Transforming Word Indices to Embeddings:** Given an input sequence of word indices, the layer converts each index to its corresponding dense embedding vector using the lookup table.\n",
    "3. **Output:** The output is a sequence of dense embedding vectors, each of dimension `output_dim`.\n",
    "\n",
    "By utilizing word embeddings, neural networks can learn semantic representations crucial for various NLP tasks.\n",
    "\n",
    "**Example:**\n",
    "For instance, if the input sequence is `[44, 31, 21, 18, 0, 0, 0, ...]`, the layer retrieves the embedding vectors for indices 44, 31, 21, 18, and so on, from its lookup table, resulting in a sequence of embedding vectors e.g., `[vector_23, vector_1, vector_999, vector_1342, vector_0,...........]`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4534c05b-4cb1-4dcb-bae7-49d5cc3e40a3",
   "metadata": {},
   "source": [
    "## Layer 2 - Convolutional Layer (Conv1D):\n",
    "The Convolutional Layer (Conv1D) is a type of layer commonly used in convolutional neural networks (CNNs) for processing one-dimensional sequences, such as time series or text data.\n",
    "\n",
    "## Parameters:\n",
    "- `filters`: Number of filters (or kernels) to apply to the input. Each filter produces one feature map in the output.\n",
    "- `kernel_size`: Size of the convolutional window, determining the number of neighboring elements considered at a time during the convolution operation.\n",
    "- `activation`: Activation function applied to the output of the convolution operation.\n",
    "\n",
    "## How it Works:\n",
    "1. **Convolution Operation:** The Conv1D layer performs a 1D convolution operation on the input sequence using the specified number of filters and kernel size. Each filter slides across the input sequence, computing dot products with the corresponding elements in the input.\n",
    "2. **Summation under the Window:** After computing the dot products, the convolution operation performs a summation under the window, aggregating the results to produce a single value for each position in the output feature map.\n",
    "3. **Feature Extraction:** The convolution operation extracts features from the input sequence, capturing patterns or local dependencies within the data.\n",
    "4. **Activation Function (ReLU):** The ReLU activation function is applied element-wise to the output of the convolution operation. It replaces any negative values with zero, introducing non-linearity and enabling the network to learn complex relationships in the data. The ReLU function is defined as:\n",
    "ReLU(x) = max(0, x)\n",
    "\n",
    "\n",
    "Convolutional layers are effective for capturing local patterns and dependencies in sequential data, making them suitable for tasks such as text classification and time series forecasting.\n",
    "\n",
    "**Example:**\n",
    "For instance, if the input sequence is `[44, 31, 21, 18, 0, 0, 0, ...]`, the Conv1D layer applies filters to extract features from the sequence and then applies the ReLU activation function to introduce non-linearity.\n",
    "\n",
    "**Output Format:**\n",
    "The output of the Conv1D layer consists of feature maps. Each feature map is a two-dimensional array, where the first dimension corresponds to the number of filters (`filters`) and the second dimension corresponds to the length of the feature map. For example, if the Conv1D layer has 64 filters and the input sequence length is 246, the output would be 64 feature maps, each with 246 values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0474d264-b5c5-4eb4-b877-ab69accbd330",
   "metadata": {},
   "source": [
    "## Layer 3 - Global Max Pooling Layer (GlobalMaxPooling1D):\n",
    "The Global Max Pooling Layer (GlobalMaxPooling1D) is a pooling layer commonly used in convolutional neural networks (CNNs) for processing one-dimensional sequences. It performs a global maximum pooling operation across the entire sequence, reducing the spatial dimensions to a single value per feature map.\n",
    "\n",
    "## How it Works:\n",
    "1. **Maximum Pooling Operation:** The GlobalMaxPooling1D layer computes the maximum value across each feature map along the temporal (sequence) dimension.\n",
    "2. **Feature Aggregation:** The maximum value for each feature map serves as a summary statistic, capturing the most salient feature within the sequence.\n",
    "3. **Output:** The output of the GlobalMaxPooling1D layer is a one-dimensional vector, with each element representing the maximum value of the corresponding feature map.\n",
    "\n",
    "Global max pooling is useful for capturing the most important features within the input sequence, often used as a dimensionality reduction technique before feeding the data into fully connected layers for further processing.\n",
    "\n",
    "**Example:**\n",
    "Suppose the input to the GlobalMaxPooling1D layer consists of 64 feature maps, each with a length of 246 values. After applying the global max pooling operation, the output would be a one-dimensional vector containing 64 values, each representing the maximum value of the corresponding feature map.\n",
    "\n",
    "**Output Format:**\n",
    "The output of the GlobalMaxPooling1D layer is a one-dimensional vector, where the length of the vector corresponds to the number of feature maps in the input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9311a21f-38d2-4641-b50e-ca07204c36db",
   "metadata": {},
   "source": [
    "## Layer 4 - Dense Layer:\n",
    "The Dense Layer is a fully connected layer in a neural network, where each neuron is connected to every neuron in the previous layer. It is commonly used for learning non-linear relationships in the data.\n",
    "\n",
    "## Parameters:\n",
    "- `units`: Number of neurons or units in the layer.\n",
    "- `activation`: Activation function applied to the output of the layer.\n",
    "\n",
    "## How it Works:\n",
    "1. **Weighted Sum Calculation:** The Dense layer computes the weighted sum of the inputs from the previous layer along with biases for each neuron.\n",
    "2. **Activation Function:** The activation function (e.g., ReLU) is applied element-wise to the output of the weighted sum, introducing non-linearity to the model.\n",
    "3. **Output:** The output of the Dense layer is a vector of size `units`, where each element represents the output of a neuron in the layer.\n",
    "\n",
    "Dense layers are versatile and can be used in various neural network architectures for tasks such as classification, regression, and more.\n",
    "\n",
    "**Example:**\n",
    "Suppose we have a Dense layer with 64 units and ReLU activation. If the input to this layer is a vector of size 100, the output would also be a vector of size 64.\n",
    "\n",
    "**Output Format:**\n",
    "The output of the Dense layer is a vector of size `units`, where each element represents the output of a neuron in the layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2baeefb-ab82-45e5-91bf-4e896a79c186",
   "metadata": {},
   "source": [
    "## Layer 5 - Dense Layer for Binary Classification:\n",
    "The Dense Layer with `sigmoid` activation is commonly used as the output layer for binary classification tasks, where the goal is to predict one of two classes (e.g., positive or negative).\n",
    "\n",
    "## Parameters:\n",
    "- `units`: Number of neurons or units in the layer.\n",
    "- `activation`: Activation function applied to the output of the layer.\n",
    "\n",
    "## How it Works:\n",
    "1. **Weighted Sum Calculation:** The Dense layer computes the weighted sum of the inputs from the previous layer along with biases for each neuron.\n",
    "2. **Activation Function (Sigmoid):** The sigmoid activation function is applied element-wise to the output of the weighted sum. It squashes the output values between 0 and 1, interpreting them as probabilities. In binary classification, a threshold (typically 0.5) is applied to these probabilities to make class predictions.\n",
    "3. **Output:** The output of the Dense layer is a single scalar value representing the probability of belonging to the positive class.\n",
    "\n",
    "Dense layers with sigmoid activation are suitable for binary classification tasks, such as sentiment analysis, spam detection, and medical diagnosis.\n",
    "\n",
    "**Example:**\n",
    "Suppose we have a Dense layer with 1 unit and sigmoid activation. If the input to this layer is a vector of size 64, the output would be a single scalar value between 0 and 1, representing the probability of the positive class.\n",
    "\n",
    "**Output Format:**\n",
    "The output of the Dense layer is a single scalar value representing the probability of belonging to the positive class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a816ac83-ea4c-4dbe-9fcb-821ed7147d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length),\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification (positive/negative)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa6d77e-3ae9-4def-ade1-bf5ffef4fb73",
   "metadata": {},
   "source": [
    "## Model Compilation:\n",
    "After defining the architecture of the neural network, the next step is to compile the model. During compilation, you specify additional settings that determine how the model will be trained.\n",
    "\n",
    "## Parameters:\n",
    "- `optimizer`: Optimization algorithm used to update the weights of the neural network during training. Adam is a popular optimizer known for its adaptive learning rate capabilities.\n",
    "- `loss`: Loss function used to measure the difference between the predicted outputs and the true labels during training. For binary classification tasks, binary cross-entropy is commonly used.\n",
    "- `metrics`: List of metrics to evaluate the performance of the model during training and testing. Accuracy is a commonly used metric that measures the proportion of correctly classified samples.\n",
    "\n",
    "## How it Works:\n",
    "1. **Optimizer Selection:** The optimizer (`adam` in this case) determines the strategy used to update the weights of the neural network based on the computed gradients during backpropagation.\n",
    "2. **Loss Function:** The loss function (`binary_crossentropy`) quantifies the difference between the predicted outputs and the true labels. It is specifically designed for binary classification tasks and penalizes deviations from the true labels.\n",
    "3. **Metrics:** During training and evaluation, the model's performance is monitored using the specified metrics (e.g., accuracy). These metrics provide insights into how well the model is performing on the given task.\n",
    "\n",
    "Model compilation is a crucial step in setting up the training process, as it defines the optimization strategy and evaluation criteria used to train and assess the model's performance.\n",
    "\n",
    "**Example:**\n",
    "In this example, the model is compiled using the Adam optimizer, binary cross-entropy loss function, and accuracy as the evaluation metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b17499c-33d7-4c75-a31c-45102128b8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96f7358-0233-4c1a-90ce-353339e5f794",
   "metadata": {},
   "source": [
    "## Model Training:\n",
    "Once the model architecture is defined and compiled, the next step is to train the model on the training data.\n",
    "\n",
    "## Parameters:\n",
    "- `train_data`: Input training data, typically features or input sequences.\n",
    "- `train_labels`: True labels corresponding to the training data.\n",
    "- `epochs`: Number of times the entire training dataset is passed forward and backward through the neural network. One epoch consists of one forward pass and one backward pass.\n",
    "- `batch_size`: Number of samples processed simultaneously by the model during training. It determines the number of samples used to compute the gradient and update the weights in each optimization step.\n",
    "- `validation_data`: Optional tuple containing validation data and labels. If provided, the model's performance on the validation set is evaluated at the end of each epoch.\n",
    "\n",
    "## How it Works:\n",
    "1. **Forward and Backward Pass:** During each epoch, the model performs a forward pass to compute the predicted outputs, followed by a backward pass to compute the gradients of the loss function with respect to the model parameters.\n",
    "2. **Gradient Descent:** The optimization algorithm (e.g., Adam) uses the computed gradients to update the weights of the neural network, minimizing the loss function and improving the model's performance.\n",
    "3. **Validation:** At the end of each epoch, if validation data is provided, the model's performance is evaluated on the validation set using the specified metrics (e.g., accuracy).\n",
    "4. **Monitoring Training Progress:** Throughout the training process, the model's performance metrics on the training and validation sets are logged, allowing for monitoring of training progress and detection of overfitting or underfitting.\n",
    "\n",
    "Training a neural network involves finding the optimal set of weights that minimize the loss function and generalize well to unseen data.\n",
    "\n",
    "**Example:**\n",
    "In this example, the model is trained for 5 epochs using a batch size of 64. Training data (`train_data`) and corresponding labels (`train_labels`) are provided. Additionally, the model's performance is evaluated on the test set (`test_data`) and test labels (`test_labels`) after each epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2fc93c1-9512-4f28-a4e3-8f4d82511e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "391/391 [==============================] - 48s 115ms/step - loss: 0.4181 - accuracy: 0.7976 - val_loss: 0.2909 - val_accuracy: 0.8776\n",
      "Epoch 2/5\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1659 - accuracy: 0.9393 - val_loss: 0.2835 - val_accuracy: 0.8834\n",
      "Epoch 3/5\n",
      "391/391 [==============================] - 49s 126ms/step - loss: 0.0406 - accuracy: 0.9907 - val_loss: 0.3162 - val_accuracy: 0.8915\n",
      "Epoch 4/5\n",
      "391/391 [==============================] - 48s 124ms/step - loss: 0.0070 - accuracy: 0.9994 - val_loss: 0.3470 - val_accuracy: 0.8934\n",
      "Epoch 5/5\n",
      "391/391 [==============================] - 54s 138ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.3796 - val_accuracy: 0.8942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x177dac46bb0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "model.fit(train_data, train_labels, epochs=epochs, batch_size=batch_size,\n",
    "          validation_data=(test_data, test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcf39d3-a332-4ec7-8aae-a31706de3272",
   "metadata": {},
   "source": [
    "## Model Evaluation:\n",
    "After training the model, it's essential to evaluate its performance on unseen data to assess its generalization capabilities.\n",
    "\n",
    "## Parameters:\n",
    "- `test_data`: Input test data, typically features or input sequences.\n",
    "- `test_labels`: True labels corresponding to the test data.\n",
    "\n",
    "## How it Works:\n",
    "1. **Forward Pass:** The model performs a forward pass on the test data to compute the predicted outputs.\n",
    "2. **Loss Computation:** The loss function is computed using the predicted outputs and the true labels.\n",
    "3. **Metric Calculation:** If specified during model compilation, additional metrics (e.g., accuracy) are calculated based on the predicted outputs and true labels.\n",
    "4. **Evaluation:** The model's performance metrics, such as loss and accuracy, are returned as output.\n",
    "\n",
    "Model evaluation provides insights into how well the trained model generalizes to unseen data and helps identify potential issues, such as overfitting or underfitting.\n",
    "\n",
    "**Example:**\n",
    "In this example, the model's performance is evaluated on the test data (`test_data`) and corresponding labels (`test_labels`). The evaluation results include the test loss and test accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d81badf9-f9f2-4019-8582-d6d2952512db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 11s 14ms/step - loss: 0.3796 - accuracy: 0.8942\n",
      "Test accuracy: 0.8942\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c0b293-de52-49f4-abd2-7edad73a0c20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
